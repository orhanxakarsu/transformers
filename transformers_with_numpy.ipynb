{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32f9cccc-d694-467a-91bf-88e549324469",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb167a08-761d-49b3-bdac-535d3bc9a146",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention:\n",
    "    def __init__(self,num_hiddens, num_heads,dropout =0.0,bias=True):\n",
    "        self.num_heads = num_heads\n",
    "        self.num_hiddens=num_hiddens\n",
    "        self.d_k = self.d_v = num_hiddens//num_heads\n",
    "\n",
    "        self.W_q = np.random.rand(num_hiddens,num_hiddens)\n",
    "        self.W_k = np.random.rand(num_hiddens,num_hiddens)\n",
    "        self.W_v = np.random.rand(num_hiddens, num_hiddens)\n",
    "        self.W_o = np.random.rand(num_hiddens,num_hiddens)\n",
    "\n",
    "        if bias:\n",
    "            self.b_q = np.random.rand(num_hiddens)\n",
    "            self.b_k = np.random.rand(num_hiddens)\n",
    "            self.b_v = np.random.rand(num_hiddens)\n",
    "            self.b_o = np.random.rand(num_hiddens)\n",
    "            \n",
    "        else:\n",
    "            self.b_q = self.b_k = self.b_o=self.b_v = np.zeros(num_hiddens)\n",
    "\n",
    "\n",
    "    def transpose_qkv(self,X):\n",
    "        X = X.reshape(X.shape[0],X.shape[1],self.num_heads,-1)\n",
    "        X = X.transpose(0,2,1,3)\n",
    "        return X.reshape(-1, X.shape[2],X.shape[3])\n",
    "\n",
    "    def transpose_output(self,X):\n",
    "        X = X.reshape(-1,self.num_heads, X.shape[1], X.shape[2])\n",
    "        X = X.transpose(0,2,1,3)\n",
    "        return X.reshape(X.shape[0],X.shape[1],-1)\n",
    "\n",
    "\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q ,K, V, valid_lens):\n",
    "        d_k = Q.shape[-1]\n",
    "        scores = np.matmul(Q, K.transpose(0,2,1))/np.sqrt(d_k)\n",
    "\n",
    "        if valid_lens is not None:\n",
    "            mask = np.arange(scores.shape[-1])<valid_lens[:,None]\n",
    "            scores = np.where(mask[:, None, :], scores, -np.inf)\n",
    "\n",
    "        attention_weights = np.exp(scores - np.max(scores,axis =-1, keepdims =True))\n",
    "        attention_weights /= attention_weights.sum(axis=-1, keepdims =True)\n",
    "        return np.matmul(attention_weights,V)\n",
    "\n",
    "\n",
    "    def forward(self, queries,keys, values, valid_lens):\n",
    "        queries = self.transpose_qkv(np.dot(queries,self.W_q) + self.b_q)\n",
    "        keys = self.transpose_qkv(np.dot(keys,self.W_k) + self.b_k)\n",
    "        values = self.transpose_qkv(np.dot(values,self.W_v) + self.b_v)\n",
    "\n",
    "        if valid_lens is not None:\n",
    "            #valid_lens maskesini head sayısı kadar tekrarla:\n",
    "            valid_lens = np.repeat(valid_lens,self.num_heads,axis=0)\n",
    "\n",
    "        output = self.scaled_dot_product_attention(queries,keys,values,valid_lens)\n",
    "        output_concat = self.transpose_output(output)\n",
    "        return np.dot(output_concat,self.W_o) + self.b_o\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3aa1851-3ba5-4d91-add1-19cf46b78875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(seq_len, d_model):\n",
    "    \n",
    "    pos = np.arange(seq_len)[:, np.newaxis]\n",
    "    i = np.arange(d_model)[np.newaxis, :]\n",
    "    \n",
    "    \n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    \n",
    "    pos_encoding = pos * angle_rates\n",
    "    \n",
    "    pos_encoding[:, 0::2] = np.sin(pos_encoding[:, 0::2])\n",
    "    pos_encoding[:, 1::2] = np.cos(pos_encoding[:, 1::2])\n",
    "    \n",
    "    return pos_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f7bf38d-c979-4328-b73d-41fba5bf3b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward:\n",
    "    def __init__(self,d_model, d_ff):\n",
    "        self.W1 = np.random.randn(d_model,d_ff)* np.sqrt(2.0/(d_model + d_ff))\n",
    "        self.b1 = np.zeros(d_ff)\n",
    "        self.W2 = np.random.randn(d_model,d_ff)* np.sqrt(2.0 / (d_ff + d_model))\n",
    "        self.b2 = np.zeros(d_model)\n",
    "\n",
    "\n",
    "\n",
    "    def __call__(self,x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return np.dot(np.maximum(0,np.dot(x,self.W1) + self.b1), self.W2) + self.b2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0f5eeae-e54b-44dd-8afa-530fdd6574ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer:\n",
    "    def __init__(self,d_model,num_heads,d_ff,dropout =0, bias=False):\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.multi_head_attention = MultiHeadAttention(d_model,num_heads,dropout,bias)\n",
    "        self.feed_forward = FeedForward(d_model,d_ff)\n",
    "\n",
    "\n",
    "    def __call__(self, x, mask=None):\n",
    "        return self.forward(x,mask)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        atnn_output = self.multi_head_attention.forward(x,x,x,mask)\n",
    "        output = self.feed_forward(atnn_output)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a408d38e-6f42-4850-86a8-ffbaaaf5adce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer:\n",
    "    def __init__(self,d_model,num_heads,d_ff,dropout=0.0,bias =False):\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.multi_head_attention_1 = MultiHeadAttention(d_model,num_heads,dropout,bias)\n",
    "        self.multi_head_attention_2 = MultiHeadAttention(d_model,num_heads,dropout,bias)\n",
    "        self.feed_forward = FeedForward(d_model,d_ff)\n",
    "\n",
    "\n",
    "    def __call__(self,x,enc_output,mask=None):\n",
    "        return self.forward(x,enc_output,mask)\n",
    "\n",
    "    def forward(self,x,enc_output,mask=None):\n",
    "        atnn_output1 = self.multi_head_attention_1(x,x,x,mask)\n",
    "        atnn_output2 = self.multi_head_attention_2(atnn_output1,enc_output,enc_output,mask)\n",
    "        output = self.feed_forward(atnn_output2)\n",
    "        return output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb42b937-e8f8-44f1-b25a-62c890b59086",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer:\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, input_vocab_size, target_vocab_size, max_seq_len):\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_ff = d_ff\n",
    "        self.num_layers = num_layers\n",
    "        self.input_vocab_size = input_vocab_size\n",
    "        self.target_vocab_size = target_vocab_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.encoder_layers = [EncoderLayer(d_model,num_heads,d_ff) for _ in range(num_layers)]\n",
    "        self.decoder_layers = [DecoderLayer(d_model,num_heads,d_ff) for _ in range(num_layers)]\n",
    "        \n",
    "\n",
    "        self.embedding = np.random.randn(input_vocab_size,d_model)* np.sqrt(2.0/(input_vocab_size + d_model))\n",
    "        self.pos_encoding = positional_encoding(max_seq_len,d_model)\n",
    "        self.output_layer = np.random.randn(d_model,target_vocab_size)* np.sqrt(2.0/(d_model + target_vocab_size))\n",
    "\n",
    "\n",
    "    def __call__(self, input_seq, target_seq, mask=None):\n",
    "        return self.forward(input_seq, target_seq, mask)\n",
    "\n",
    "    def forward(self, input_seq, target_seq, mask=None):\n",
    "        enc_output = self.encode(input_seq, mask)\n",
    "        dec_output = self.decode(target_seq, enc_output, mask)\n",
    "        output = np.dot(dec_output, self.output_layer)\n",
    "        return output\n",
    "\n",
    "\n",
    "    def encode(self, input_seq, mask=None):\n",
    "        seq_len = input_seq.shape[1]\n",
    "        #print(input_seq)\n",
    "        x = self.embedding[input_seq] + self.pos_encoding[:seq_len, :]\n",
    "        \n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x, mask)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "    def decode(self, target_seq, enc_output, mask =None):\n",
    "        seq_len = target_seq.shape[1]\n",
    "        x = self.embedding[target_seq] + self.pos_encoding[:seq_len, :]\n",
    "\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x,enx_output, mask)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b44db27-6d87-4f0b-9eaa-8ea753fb39f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define some parameters\n",
    "d_model = 32\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "num_layers = 6\n",
    "input_vocab_size = 10000\n",
    "target_vocab_size = 10000\n",
    "max_seq_len =100\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f498d9b8-8512-451d-8a99-41c307010093",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(d_model, num_heads, d_ff, num_layers, input_vocab_size, target_vocab_size, max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0bd72778-93ba-491d-981a-88742bb7680a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq = np.random.randint(0, input_vocab_size, (50, 32))\n",
    "target_seq = np.random.randint(0, target_vocab_size, (50, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcde4b2d-0466-4e20-991a-bc08e18a07c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = transformer(input_seq,target_seq)\n",
    "print(output.shape) # (batch_size, target_seq_len, target_vocab_size) olmalı"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c57b475-0b84-48d1-9350-ae60f38fe914",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
